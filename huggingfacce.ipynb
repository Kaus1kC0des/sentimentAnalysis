{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-17T13:25:06.484880600Z",
     "start_time": "2023-09-17T13:23:54.254514500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 3)) (0.12.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 5)) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn-intelex in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 7)) (2023.2.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 8)) (4.33.2)\n",
      "Collecting text_hammer (from -r requirements.txt (line 9))\n",
      "  Using cached text_hammer-0.1.5-py3-none-any.whl (7.6 kB)\n",
      "Collecting spacy (from -r requirements.txt (line 10))\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/d6/9e/8afc618cfed4b5dc602b11754d4d9193a268439704defae301bffca7f04c/spacy-3.6.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached spacy-3.6.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting tensorflow (from -r requirements.txt (line 11))\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/9e/b8/ed5f794359d05cd0bffb894c6418da87b93016ee17b669d55c45d1bd5d5b/tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: opendatasets in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 12)) (0.1.22)\n",
      "Collecting datasets (from -r requirements.txt (line 13))\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from -r requirements.txt (line 14)) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: daal4py==2023.2.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from scikit-learn-intelex->-r requirements.txt (line 7)) (2023.2.1)\n",
      "Requirement already satisfied: daal==2023.2.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from daal4py==2023.2.1->scikit-learn-intelex->-r requirements.txt (line 7)) (2023.2.1)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from daal==2023.2.1->daal4py==2023.2.1->scikit-learn-intelex->-r requirements.txt (line 7)) (2021.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (0.17.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4==4.9.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from text_hammer->-r requirements.txt (line 9)) (4.9.1)\n",
      "Requirement already satisfied: TextBlob in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from text_hammer->-r requirements.txt (line 9)) (0.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from beautifulsoup4==4.9.1->text_hammer->-r requirements.txt (line 9)) (2.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (68.0.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from spacy->-r requirements.txt (line 10)) (3.3.0)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/2f/2f/3c84f675931ce3bcbc7e23acbba1e5d7f05ce769adab48322de57a9f5928/tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (4.24.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (1.58.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (0.31.0)\n",
      "Requirement already satisfied: kaggle in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from opendatasets->-r requirements.txt (line 12)) (1.5.16)\n",
      "Requirement already satisfied: click in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from opendatasets->-r requirements.txt (line 12)) (8.1.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from datasets->-r requirements.txt (line 13)) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from datasets->-r requirements.txt (line 13)) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from datasets->-r requirements.txt (line 13)) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from datasets->-r requirements.txt (line 13)) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from datasets->-r requirements.txt (line 13)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from datasets->-r requirements.txt (line 13)) (3.8.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from torch->-r requirements.txt (line 14)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from torch->-r requirements.txt (line 14)) (3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 13)) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 13)) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 13)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 13)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 13)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 13)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from aiohttp->datasets->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 8)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 8)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 8)) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 10)) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 10)) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tqdm>=4.27->transformers->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from jinja2->spacy->-r requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from kaggle->opendatasets->-r requirements.txt (line 12)) (8.0.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from kaggle->opendatasets->-r requirements.txt (line 12)) (4.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from TextBlob->text_hammer->-r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (2.3.7)\n",
      "Requirement already satisfied: webencodings in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from bleach->kaggle->opendatasets->-r requirements.txt (line 12)) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from python-slugify->kaggle->opendatasets->-r requirements.txt (line 12)) (1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 11)) (3.2.2)\n",
      "Using cached spacy-3.6.1-cp311-cp311-win_amd64.whl (12.0 MB)\n",
      "Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl (276.6 MB)\n",
      "Using cached datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "Installing collected packages: spacy, datasets, text_hammer, tensorflow-intel, tensorflow\n",
      "Successfully installed datasets-2.14.5 spacy-3.6.1 tensorflow-2.13.0 tensorflow-intel-2.13.0 text_hammer-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading sentiment140.zip to .\\sentiment140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80.9M/80.9M [00:04<00:00, 18.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/kazanova/sentiment140\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:25:30.055927300Z",
     "start_time": "2023-09-17T19:24:55.274248900Z"
    }
   },
   "id": "24baa191e81bc95e"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "   sentiment          id                          date     query  \\\n0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n\n              user                                               text  \n0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1    scotthamilton  is upset that he can't update his Facebook by ...  \n2         mattycus  @Kenichan I dived many times for the ball. Man...  \n3          ElleCTF    my whole body feels itchy and like its on fire   \n4           Karoli  @nationwideclass no, it's not behaving at all....  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>id</th>\n      <th>date</th>\n      <th>query</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\sentiment140\\training.1600000.processed.noemoticon.csv\",names=[\"sentiment\",\"id\",\"date\",\"query\",\"user\",\"text\"],encoding='latin-1')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:38.687862100Z",
     "start_time": "2023-09-17T19:59:33.016813600Z"
    }
   },
   "id": "932b49979d9817b1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "sentiment    0\nid           0\ndate         0\nquery        0\nuser         0\ntext         0\ndtype: int64"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:38.969091Z",
     "start_time": "2023-09-17T19:59:38.678661100Z"
    }
   },
   "id": "ffa7ff97050dc84c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df.drop([\"id\",\"date\",\"query\",\"user\"],axis=1,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:38.993575500Z",
     "start_time": "2023-09-17T19:59:38.942242700Z"
    }
   },
   "id": "6ab08b23077f4974"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   sentiment                                               text\n0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1          0  is upset that he can't update his Facebook by ...\n2          0  @Kenichan I dived many times for the ball. Man...\n3          0    my whole body feels itchy and like its on fire \n4          0  @nationwideclass no, it's not behaving at all....",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:39.021367800Z",
     "start_time": "2023-09-17T19:59:39.000347200Z"
    }
   },
   "id": "d6f036ab942a77a4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer,pipeline\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "sentiment = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:50.158902700Z",
     "start_time": "2023-09-17T19:59:39.005710200Z"
    }
   },
   "id": "b0fe4ddc112710fe"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_2', 'score': 0.9876530170440674}]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"I am very happy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:50.219459300Z",
     "start_time": "2023-09-17T19:59:50.158902700Z"
    }
   },
   "id": "36cc7a8288f377e2"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_0', 'score': 0.949698805809021}]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"I am very sad\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:50.298080500Z",
     "start_time": "2023-09-17T19:59:50.204420400Z"
    }
   },
   "id": "bc32b3a6b06604f0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_0', 'score': 0.9702324867248535}]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"I am very angry\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:50.298080500Z",
     "start_time": "2023-09-17T19:59:50.250729400Z"
    }
   },
   "id": "dd104be9dae1ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm -q\n",
    "# Run this if you don't have spacy installed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:50.298080500Z",
     "start_time": "2023-09-17T19:59:50.281969500Z"
    }
   },
   "id": "c24f2404543b9aa2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import text_hammer as th\n",
    "# from tqdm._tqdm_notebook import tqdm_notebook\n",
    "# tqdm_notebook.pandas()\n",
    "# def text_preprocessing(df,col_name):\n",
    "#     column = col_name\n",
    "#     df[column] = df[column].progress_apply(lambda x:str(x).lower())\n",
    "#     #     df[column] = df[column].progress_apply(lambda x: th.cont_exp(x))\n",
    "#     #you're -> you are; i'm -> i am\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n",
    "#     return(df)\n",
    "# \n",
    "# df['text'] = text_preprocessing(df,'text')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:51.000689300Z",
     "start_time": "2023-09-17T19:59:50.998585Z"
    }
   },
   "id": "7efdfb237c2c1c1a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# import time\n",
    "# \n",
    "# def text_preprocessing_worker(text):\n",
    "#     \"\"\"Preprocess text using a specific function.\"\"\"\n",
    "# \n",
    "#     # Preprocess the text using your chosen function\n",
    "#     preprocessed_text = th.cont_exp(text)\n",
    "# \n",
    "#     return preprocessed_text\n",
    "# \n",
    "# # Create a pool of workers to execute the text preprocessing function in parallel\n",
    "# start = time.time()\n",
    "# pool = multiprocessing.Pool(4)\n",
    "# \n",
    "# # Apply the text preprocessing function to each text in the DataFrame using the pool of workers\n",
    "# preprocessed_texts = pool.map(text_preprocessing_worker, df['text'])\n",
    "# \n",
    "# # Add the preprocessed texts to the DataFrame\n",
    "# df['text'] = preprocessed_texts\n",
    "# \n",
    "# # Close the pool of workers\n",
    "# pool.close()\n",
    "# print(time.time() - start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:59:52.540122700Z",
     "start_time": "2023-09-17T19:59:52.531818500Z"
    }
   },
   "id": "fe0adfac35b294b7"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1600000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b762485ab13b46b0954fe09352ca3243"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1600000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96eff0447c3942b9a50388ebdfce93bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1600000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f63fd82c482454f908210d0dd91802f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1600000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e16eacbf29b04f7791338202fe791d90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1600000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aa2377367b848029e69f538b6a3f09c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1600000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6958f0d4377f4fa882c88955894c42a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\magics\\execution.py\", line 1340, in time\n",
      "    exec(code, glob, local_ns)\n",
      "  File \"<timed exec>\", line 36, in <module>\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4084, in __setitem__\n",
      "    self._set_item_frame_value(key, value)\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4212, in _set_item_frame_value\n",
      "    raise ValueError(\"Columns must be same length as key\")\n",
      "ValueError: Columns must be same length as key\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1155, in get_records\n",
      "    FrameInfo(\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 780, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\inspect.py\", line 1244, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kausikdevanathan\\anaconda3\\envs\\SIH\\Lib\\inspect.py\", line 1081, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import unicodedata\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "def vectorized_text_preprocessing_tqdm(df, col_name):\n",
    "    \"\"\"\n",
    "    Performs text preprocessing on a Pandas DataFrame column using vectorized operations and shows the progress bar.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame.\n",
    "        col_name: The name of the column to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the preprocessed column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Show the progress bar.\n",
    "    tqdm_notebook.pandas()\n",
    "\n",
    "    # Convert the column to lowercase.\n",
    "    df[col_name] = df[col_name].progress_apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove emails, HTML tags, special characters, and accented characters.\n",
    "    df[col_name] = df[col_name].progress_apply(lambda x: x.replace(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", \"\"))\n",
    "    df[col_name] = df[col_name].progress_apply(lambda x: x.replace(r\"<[^>]*>\", \"\"))\n",
    "    df[col_name] = df[col_name].progress_apply(lambda x: x.replace(r\"[^\\w\\s]\", \"\"))\n",
    "    df[col_name] = df[col_name].progress_apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", errors=\"ignore\").decode(\"ascii\"))\n",
    "\n",
    "    # Make the text base form.\n",
    "    df[col_name] = df[col_name].progress_apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the text column using the vectorized approach with progress bar.\n",
    "df['text'] = vectorized_text_preprocessing_tqdm(df, \"text\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:00:11.697276400Z",
     "start_time": "2023-09-17T19:59:54.293945100Z"
    }
   },
   "id": "a12e77177f2db6c4"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:00:11.705089500Z",
     "start_time": "2023-09-17T20:00:11.701355100Z"
    }
   },
   "id": "4e5d8c632755b7ec"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\sentiment140\\preprocessed.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:00:18.475706800Z",
     "start_time": "2023-09-17T20:00:11.705089500Z"
    }
   },
   "id": "fdd9561484c9fb51"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "sentiment\n0    800000\n4    800000\nName: count, dtype: int64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:00:18.498765300Z",
     "start_time": "2023-09-17T20:00:18.481070300Z"
    }
   },
   "id": "efa9e9bd66b802da"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Split the test-validation set into test and validation sets\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.50, random_state=42)\n",
    "\n",
    "# Save the train, test, and validation sets to disk\n",
    "X_train.to_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\train.csv\", index=False)\n",
    "X_test.to_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\test.csv\", index=False)\n",
    "X_val.to_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\validation.csv\", index=False)\n",
    "y_train.to_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\train_labels.csv\", index=False)\n",
    "y_test.to_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\test_labels.csv\", index=False)\n",
    "y_val.to_csv(r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\validation_labels.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:37:03.602238600Z",
     "start_time": "2023-09-17T19:36:55.186007300Z"
    }
   },
   "id": "dc1303620ecfc998"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\train.csv\",\n",
    "        \"test\": r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\test.csv\",\n",
    "        \"validation\": r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\validation.csv\",\n",
    "    },\n",
    "    column_names=[\"text\", \"label\"],\n",
    "    cache_dir=\"\",\n",
    ")\n",
    "dataset.full_determinism = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:00:27.508439100Z",
     "start_time": "2023-09-17T20:00:25.845904600Z"
    }
   },
   "id": "b653bfb2a4891045"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 1120001\n})"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:00:27.516214800Z",
     "start_time": "2023-09-17T20:00:27.509405400Z"
    }
   },
   "id": "5651b260c751d491"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'full_determinism'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Trainer\n\u001B[0;32m      3\u001B[0m dataset\u001B[38;5;241m.\u001B[39mfull_determinism \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m      5\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_path,\n\u001B[0;32m      6\u001B[0m     args\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_train_epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m5e-5\u001B[39m,\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluation_strategy\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     10\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mper_device_train_batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m32\u001B[39m,\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mper_device_eval_batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m32\u001B[39m,\n\u001B[0;32m     12\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging_steps\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.01\u001B[39m,\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload_best_model_at_end\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     15\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetric_for_best_model\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     16\u001B[0m     },\n\u001B[0;32m     17\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mdataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     18\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mdataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     19\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mmodel_path,\n\u001B[0;32m     20\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer.py:337\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    336\u001B[0m \u001B[38;5;66;03m# Seed must be set before instantiating the model when using model\u001B[39;00m\n\u001B[1;32m--> 337\u001B[0m enable_full_determinism(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mseed) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mfull_determinism \u001B[38;5;28;01melse\u001B[39;00m set_seed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mseed)\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhp_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'dict' object has no attribute 'full_determinism'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "dataset.full_determinism = False\n",
    "trainer = Trainer(\n",
    "    model=model_path,\n",
    "    args={\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"logging_steps\": 100,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"accuracy\",\n",
    "    },\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=model_path,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:00:32.677198200Z",
     "start_time": "2023-09-17T20:00:30.893024800Z"
    }
   },
   "id": "dd6942170cd007e8"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_ids,attention_mask,token_type_ids,position_ids,head_mask,inputs_embeds,labels,output_attentions,output_hidden_states,return_dict,labels,label_ids,label.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 29\u001B[0m\n\u001B[0;32m     21\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     22\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     23\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m     24\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mdataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     25\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mdataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     26\u001B[0m )\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer.py:1553\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[0;32m   1554\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[0;32m   1555\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[0;32m   1556\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[0;32m   1557\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[0;32m   1558\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer.py:1835\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1832\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   1834\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 1835\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[0;32m   1837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1838\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   1839\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[0;32m   1840\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   1841\u001B[0m ):\n\u001B[0;32m   1842\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   1843\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer.py:2672\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m   2654\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2655\u001B[0m \u001B[38;5;124;03mPerform a training step on a batch of inputs.\u001B[39;00m\n\u001B[0;32m   2656\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2669\u001B[0m \u001B[38;5;124;03m    `torch.Tensor`: The tensor with training loss on this batch.\u001B[39;00m\n\u001B[0;32m   2670\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2671\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m-> 2672\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_inputs(inputs)\n\u001B[0;32m   2674\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[0;32m   2675\u001B[0m     loss_mb \u001B[38;5;241m=\u001B[39m smp_forward_backward(model, inputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer.py:2622\u001B[0m, in \u001B[0;36mTrainer._prepare_inputs\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   2620\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_input(inputs)\n\u001B[0;32m   2621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2622\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2623\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe batch received was empty, your model won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be able to train on it. Double-check that your \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2624\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining dataset contains keys expected by the model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signature_columns)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2625\u001B[0m     )\n\u001B[0;32m   2626\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_past \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2627\u001B[0m     inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmems\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_past\n",
      "\u001B[1;31mValueError\u001B[0m: The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_ids,attention_mask,token_type_ids,position_ids,head_mask,inputs_embeds,labels,output_attentions,output_hidden_states,return_dict,labels,label_ids,label."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Set the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Create a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:02:07.314447400Z",
     "start_time": "2023-09-17T20:02:04.850201600Z"
    }
   },
   "id": "d6510c1939f474ee"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m accelerate\u001B[38;5;241m.\u001B[39minit_on_device(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Set the training arguments\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m      6\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      7\u001B[0m     evaluation_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      8\u001B[0m     save_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m     10\u001B[0m     per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m,\n\u001B[0;32m     11\u001B[0m     per_device_eval_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m,\n\u001B[0;32m     12\u001B[0m     logging_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m     13\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m,\n\u001B[0;32m     14\u001B[0m     load_best_model_at_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     15\u001B[0m     metric_for_best_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Create a trainer\u001B[39;00m\n\u001B[0;32m     19\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     20\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     21\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m     22\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mdataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     23\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mdataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     24\u001B[0m )\n",
      "File \u001B[1;32m<string>:114\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\training_args.py:1405\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1399\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mbase_version) \u001B[38;5;241m==\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16:\n\u001B[0;32m   1400\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1403\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[1;32m-> 1405\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1406\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1407\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (get_xla_device_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1408\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n\u001B[0;32m   1409\u001B[0m ):\n\u001B[0;32m   1410\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1411\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1412\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1413\u001B[0m     )\n\u001B[0;32m   1415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1416\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1417\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1422\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n\u001B[0;32m   1423\u001B[0m ):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\training_args.py:1852\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[0;32m   1850\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1851\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m-> 1852\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setup_devices\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\utils\\generic.py:54\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[1;34m(self, obj, objtype)\u001B[0m\n\u001B[0;32m     52\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 54\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfget(obj)\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\training_args.py:1767\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[0;32m   1766\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available(min_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.20.1\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1767\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m   1768\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1769\u001B[0m         )\n\u001B[0;32m   1770\u001B[0m     AcceleratorState\u001B[38;5;241m.\u001B[39m_reset_state(reset_partial_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1771\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistributed_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import accelerate\n",
    "accelerate.init_on_device(\"CPU\")\n",
    "# Set the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Create a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:58:45.907563600Z",
     "start_time": "2023-09-17T19:58:45.747907900Z"
    }
   },
   "id": "43727134d7f3790e"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (0.22.0)\n",
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/d9/92/2d3aecf9f4a192968035880be3e2fc8b48d541c7128f7c936f430d6f96da/accelerate-0.23.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from accelerate) (0.17.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kausikdevanathan\\anaconda3\\envs\\sih\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "   ---------------------------------------- 0.0/258.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/258.1 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 112.6/258.1 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 258.1/258.1 kB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.22.0\n",
      "    Uninstalling accelerate-0.22.0:\n",
      "      Successfully uninstalled accelerate-0.22.0\n",
      "Successfully installed accelerate-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T19:58:37.177153700Z",
     "start_time": "2023-09-17T19:58:30.488553500Z"
    }
   },
   "id": "1e5172d50d5217d9"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 1120001\n})"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:16:09.110757Z",
     "start_time": "2023-09-17T20:16:09.079513600Z"
    }
   },
   "id": "322ca9bee8baa343"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# dataset = datasets.load_dataset(\"sentiment140\", split=\"train\")\n",
    "# import datasets\n",
    "\n",
    "# dataset = datasets.load_dataset(\"sentiment140\",encode=\"latin-1\", split=\"train\")\n",
    "dataset = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\train.csv\",\n",
    "        \"test\": r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\test.csv\",\n",
    "        \"validation\": r\"C:\\Users\\kausikdevanathan\\DataspellProjects\\sentimentAnalysis\\train_val_test_data\\validation.csv\",\n",
    "    },\n",
    "    column_names=[\"text\", \"label\"],\n",
    "    cache_dir=\"\",\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\",padding=True, truncation=True, max_length=128)\n",
    "\n",
    "preprocessed_dataset = []\n",
    "for tweet, label in zip(dataset[\"train\"][\"text\"], dataset[\"validation\"][\"label\"]):\n",
    "    input_ids = tokenizer(tweet, return_tensors=\"pt\").input_ids\n",
    "    preprocessed_dataset.append({\"input_ids\": input_ids, \"labels\": label})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:24:10.876718Z",
     "start_time": "2023-09-17T20:23:03.885622200Z"
    }
   },
   "id": "65fa35057d8350e5"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 39] at entry 0 and [1, 13] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 22\u001B[0m\n\u001B[0;32m      3\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m      4\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      5\u001B[0m     evaluation_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m     metric_for_best_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     16\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     17\u001B[0m model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     18\u001B[0m args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m     19\u001B[0m train_dataset\u001B[38;5;241m=\u001B[39mpreprocessed_dataset,\n\u001B[0;32m     20\u001B[0m )\n\u001B[1;32m---> 22\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer.py:1553\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[0;32m   1554\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[0;32m   1555\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[0;32m   1556\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[0;32m   1557\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[0;32m   1558\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer.py:1813\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1810\u001B[0m     rng_to_sync \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1812\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1813\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   1814\u001B[0m     total_batched_samples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1815\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rng_to_sync:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\accelerate\\data_loader.py:384\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 384\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(dataloader_iter)\n\u001B[0;32m    385\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\trainer_utils.py:707\u001B[0m, in \u001B[0;36mRemoveColumnsCollator.__call__\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[\u001B[38;5;28mdict\u001B[39m]):\n\u001B[0;32m    706\u001B[0m     features \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_remove_columns(feature) \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[1;32m--> 707\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_collator(features)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\data\\data_collator.py:70\u001B[0m, in \u001B[0;36mdefault_data_collator\u001B[1;34m(features, return_tensors)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# have the same attributes.\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# on the whole batch.\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch_default_data_collator(features)\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf_default_data_collator(features)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIH\\Lib\\site-packages\\transformers\\data\\data_collator.py:132\u001B[0m, in \u001B[0;36mtorch_default_data_collator\u001B[1;34m(features)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 132\u001B[0m         batch[k] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack([f[k] \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m features])\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m    134\u001B[0m         batch[k] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(np\u001B[38;5;241m.\u001B[39mstack([f[k] \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m features]))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [1, 39] at entry 0 and [1, 13] at entry 1"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=preprocessed_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:24:17.316860400Z",
     "start_time": "2023-09-17T20:24:16.044458900Z"
    }
   },
   "id": "2ed7aca430fecee0"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:32:02.838464200Z",
     "start_time": "2023-09-17T20:32:02.796050800Z"
    }
   },
   "id": "61a1c5c45aadca78"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:32:11.430353700Z",
     "start_time": "2023-09-17T20:32:11.396796700Z"
    }
   },
   "id": "5ceaffea41a9ea4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a1388ac0cebe2745"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
